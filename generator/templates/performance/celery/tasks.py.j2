"""
Celery tasks for {{ app_name }} app.

Generated by Django Enhanced Generator on {{ now().strftime('%Y-%m-%d %H:%M:%S') }}.
"""
from celery import shared_task
from celery.utils.log import get_task_logger
from django.core.mail import send_mail
from django.core.cache import cache
from django.utils import timezone
from django.db import transaction
from datetime import timedelta
import requests
import csv
import io
{% for model in models %}
from .models import {{ model.name }}
{% endfor %}

logger = get_task_logger(__name__)


{% for task in tasks %}
@shared_task(bind=True{% if task.retry_policy %}, max_retries={{ task.retry_policy.max_retries }}{% endif %}{% if task.rate_limit %}, rate_limit='{{ task.rate_limit }}'{% endif %})
def {{ task.name }}(self{% for param in task.params %}, {{ param }}{% endfor %}):
    """
    {{ task.description }}
    
    {% if task.params %}Args:
    {% for param in task.params %}    {{ param }}: Parameter description
    {% endfor %}{% endif %}
    """
    try:
        {% if task.task_type == 'import' %}
        # Import task implementation
        from .utils import BulkImporter
        
        logger.info(f"Starting import from {file_path}")
        
        importer = BulkImporter({{ task.model_name }}, file_path)
        result = importer.process()
        
        # Send notification email
        if result['success_count'] > 0:
            send_import_notification.delay(
                '{{ task.model_name }}',
                result['success_count'],
                result['error_count']
            )
        
        logger.info(
            f"Import completed: {result['success_count']} success, "
            f"{result['error_count']} errors"
        )
        
        return result
        
        {% elif task.task_type == 'export' %}
        # Export task implementation
        from .utils import ReportGenerator
        
        logger.info(f"Generating {{ task.model_name }} report with filters: {filters}")
        
        generator = ReportGenerator({{ task.model_name }}, filters)
        report_path = generator.create()
        
        # Cache report for quick access
        cache_key = f"report_{{ task.model_name.lower() }}_{generator.report_id}"
        cache.set(cache_key, report_path, 3600)  # 1 hour
        
        logger.info(f"Report generated: {report_path}")
        
        return {
            'report_id': generator.report_id,
            'path': report_path,
            'cache_key': cache_key
        }
        
        {% elif task.task_type == 'sync' %}
        # Sync task implementation
        instance = {{ task.model_name }}.objects.get(pk=instance_id)
        
        logger.info(f"Syncing {{ task.model_name }} {instance_id} with external API")
        
        # Prepare data for API
        sync_data = instance.to_api_format() if hasattr(instance, 'to_api_format') else {
            'id': instance.pk,
            # Add relevant fields
        }
        
        # Make API call
        response = requests.post(
            'https://api.example.com/sync/{{ task.model_name.lower() }}',
            json=sync_data,
            timeout=30,
            headers={'Authorization': 'Bearer YOUR_API_TOKEN'}
        )
        
        if response.status_code == 200:
            instance.sync_status = 'completed'
            instance.last_sync = timezone.now()
            instance.save()
            
            logger.info(f"Successfully synced {{ task.model_name }} {instance_id}")
            return {'status': 'success', 'response': response.json()}
        else:
            logger.error(f"Sync failed for {{ task.model_name }} {instance_id}: {response.text}")
            raise Exception(f"API sync failed with status {response.status_code}")
        
        {% elif task.task_type == 'cleanup' %}
        # Cleanup task implementation
        cutoff_date = timezone.now() - timedelta(days=days_old)
        
        logger.info(f"Cleaning up {{ task.model_name }} records older than {cutoff_date}")
        
        # Find records to delete
        queryset = {{ task.model_name }}.objects.filter(
            created_at__lt=cutoff_date,
            {% if task.model_name in models|selectattr('features.soft_delete')|map(attribute='name') %}
            is_deleted=True
            {% else %}
            # Add your cleanup criteria here
            {% endif %}
        )
        
        count = queryset.count()
        
        if count > 0:
            # Delete in batches to avoid memory issues
            batch_size = 1000
            deleted_total = 0
            
            while queryset.exists():
                batch_ids = list(queryset.values_list('id', flat=True)[:batch_size])
                deleted_count = {{ task.model_name }}.objects.filter(id__in=batch_ids).delete()[0]
                deleted_total += deleted_count
                
                logger.info(f"Deleted batch of {deleted_count} {{ task.model_name }} records")
        
        logger.info(f"Cleanup completed: deleted {deleted_total} {{ task.model_name }} records")
        
        return {'deleted_count': deleted_total}
        
        {% else %}
        # Custom task implementation
        logger.info(f"Executing custom task: {{ task.name }}")
        
        # Add your custom task logic here
        result = {'status': 'completed'}
        
        return result
        {% endif %}
        
    except Exception as exc:
        logger.error(f"Task {{ task.name }} failed: {exc}")
        {% if task.retry_policy %}
        # Retry with exponential backoff
        raise self.retry(exc=exc, countdown={{ task.retry_policy.countdown }} * (2 ** self.request.retries))
        {% else %}
        raise
        {% endif %}


{% endfor %}

# Utility tasks
@shared_task
def send_import_notification(model_name, success_count, error_count):
    """Send notification about import completion."""
    subject = f"{model_name} Import Completed"
    message = f"""
    Import completed for {model_name}:
    - Successfully imported: {success_count} records
    - Errors: {error_count} records
    """
    
    # Send to administrators
    from django.conf import settings
    admin_emails = [admin[1] for admin in getattr(settings, 'ADMINS', [])]
    
    if admin_emails:
        send_mail(
            subject=subject,
            message=message,
            from_email=settings.DEFAULT_FROM_EMAIL,
            recipient_list=admin_emails,
            fail_silently=True
        )


@shared_task
def cleanup_old_cache_entries():
    """Clean up old cache entries."""
    # This would depend on your cache backend
    # For Redis, you could use pattern matching
    logger.info("Cleaning up old cache entries")
    
    try:
        # Example for Redis
        from django_redis import get_redis_connection
        redis_conn = get_redis_connection("default")
        
        # Delete keys older than 1 day
        # This is a simplified example
        keys = redis_conn.keys("*")
        deleted = 0
        
        for key in keys:
            ttl = redis_conn.ttl(key)
            if ttl > 86400:  # 1 day
                redis_conn.delete(key)
                deleted += 1
        
        logger.info(f"Deleted {deleted} old cache entries")
        return {'deleted_keys': deleted}
        
    except Exception as e:
        logger.error(f"Cache cleanup failed: {e}")
        return {'error': str(e)}


@shared_task
def update_search_index():
    """Update search index for all models."""
    {% if features.performance.elasticsearch %}
    from django_elasticsearch_dsl.registries import registry
    
    logger.info("Updating search index")
    
    updated_models = []
    
    {% for model in models %}
    try:
        # Update {{ model.name }} index
        {{ model.name }}.objects.all().update_search_index()
        updated_models.append('{{ model.name }}')
        logger.info("Updated {{ model.name }} search index")
    except Exception as e:
        logger.error(f"Failed to update {{ model.name }} search index: {e}")
    
    {% endfor %}
    
    return {'updated_models': updated_models}
    {% else %}
    logger.info("Search indexing not configured")
    return {'message': 'Search indexing not configured'}
    {% endif %}


@shared_task
def generate_daily_reports():
    """Generate daily reports for all models."""
    from datetime import date
    
    today = date.today()
    logger.info(f"Generating daily reports for {today}")
    
    reports = {}
    
    {% for model in models %}
    # {{ model.name }} daily report
    try:
        count = {{ model.name }}.objects.filter(created_at__date=today).count()
        reports['{{ model.name.lower() }}_created_today'] = count
    except Exception as e:
        logger.error(f"Failed to generate {{ model.name }} report: {e}")
        reports['{{ model.name.lower() }}_created_today'] = 0
    
    {% endfor %}
    
    # Send report email
    if reports:
        send_daily_report.delay(reports, today.isoformat())
    
    return reports


@shared_task
def send_daily_report(reports, date_str):
    """Send daily report email."""
    subject = f"Daily Report - {date_str}"
    
    message_lines = [f"Daily Report for {date_str}:", ""]
    
    for key, value in reports.items():
        model_name = key.replace('_created_today', '').replace('_', ' ').title()
        message_lines.append(f"- {model_name}: {value} created")
    
    message = "\n".join(message_lines)
    
    # Send to administrators
    from django.conf import settings
    admin_emails = [admin[1] for admin in getattr(settings, 'ADMINS', [])]
    
    if admin_emails:
        send_mail(
            subject=subject,
            message=message,
            from_email=settings.DEFAULT_FROM_EMAIL,
            recipient_list=admin_emails,
            fail_silently=True
        )


# Bulk operation tasks
{% for model in models %}
@shared_task
def bulk_update_{{ model.name.lower() }}(ids, update_data):
    """Bulk update {{ model.name }} records."""
    logger.info(f"Bulk updating {len(ids)} {{ model.name }} records")
    
    try:
        with transaction.atomic():
            updated = {{ model.name }}.objects.filter(id__in=ids).update(**update_data)
            
            # Clear cache for updated records
            cache_keys = [f"{{ model.name.lower() }}:{id}" for id in ids]
            cache.delete_many(cache_keys)
            
            logger.info(f"Successfully updated {updated} {{ model.name }} records")
            return {'updated_count': updated}
            
    except Exception as e:
        logger.error(f"Bulk update failed for {{ model.name }}: {e}")
        raise


@shared_task
def bulk_delete_{{ model.name.lower() }}(ids):
    """Bulk delete {{ model.name }} records."""
    logger.info(f"Bulk deleting {len(ids)} {{ model.name }} records")
    
    try:
        with transaction.atomic():
            {% if model.get('features', {}).get('soft_delete') %}
            # Soft delete
            updated = {{ model.name }}.objects.filter(id__in=ids).update(
                is_deleted=True,
                deleted_at=timezone.now()
            )
            {% else %}
            # Hard delete
            deleted, _ = {{ model.name }}.objects.filter(id__in=ids).delete()
            updated = deleted
            {% endif %}
            
            # Clear cache for deleted records
            cache_keys = [f"{{ model.name.lower() }}:{id}" for id in ids]
            cache.delete_many(cache_keys)
            
            logger.info(f"Successfully deleted {updated} {{ model.name }} records")
            return {'deleted_count': updated}
            
    except Exception as e:
        logger.error(f"Bulk delete failed for {{ model.name }}: {e}")
        raise


{% endfor %}

# Monitoring and health check tasks
@shared_task
def health_check():
    """Perform health check of the system."""
    health_status = {
        'timestamp': timezone.now().isoformat(),
        'status': 'healthy',
        'checks': {}
    }
    
    # Database check
    try:
        from django.db import connection
        with connection.cursor() as cursor:
            cursor.execute("SELECT 1")
        health_status['checks']['database'] = 'ok'
    except Exception as e:
        health_status['checks']['database'] = f'error: {str(e)}'
        health_status['status'] = 'unhealthy'
    
    # Cache check
    try:
        cache.set('health_check', 'ok', 1)
        cache.get('health_check')
        health_status['checks']['cache'] = 'ok'
    except Exception as e:
        health_status['checks']['cache'] = f'error: {str(e)}'
        health_status['status'] = 'unhealthy'
    
    # Model checks
    {% for model in models %}
    try:
        {{ model.name }}.objects.count()
        health_status['checks']['{{ model.name.lower() }}_model'] = 'ok'
    except Exception as e:
        health_status['checks']['{{ model.name.lower() }}_model'] = f'error: {str(e)}'
        health_status['status'] = 'unhealthy'
    
    {% endfor %}
    
    logger.info(f"Health check completed: {health_status['status']}")
    return health_status